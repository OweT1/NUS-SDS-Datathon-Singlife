{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "#%pip install scikit-learn\n",
    "#%pip install imblearn\n",
    "#%pip install tensorflow\n",
    "#%pip install keras-tuner -q\n",
    "#%pip install imbalanced-learn\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owent\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Owent\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 52\u001b[0m\n\u001b[0;32m     39\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mBayesianOptimization(\n\u001b[0;32m     40\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39mbuild_model,\n\u001b[0;32m     41\u001b[0m     objective\u001b[38;5;241m=\u001b[39mkeras_tuner\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelloworld\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m build_model(keras_tuner\u001b[38;5;241m.\u001b[39mHyperParameters())\n\u001b[1;32m---> 52\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(x_train_new, y_train_new, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, validation_data \u001b[38;5;241m=\u001b[39m (x, y))\n\u001b[0;32m     54\u001b[0m tuner\u001b[38;5;241m.\u001b[39mresults_summary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_new' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "## Neural Network model\n",
    "\n",
    "def existing_model(units, dropout, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units = units, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(12, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy', 'F1Score', 'R2Score', keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "## Hyper parameters to be investigated\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(\"units\", min_value=24, max_value=60, step=4)\n",
    "    dropout = hp.Boolean(\"dropout\")\n",
    "    lr = hp.Float(\"lr\", min_value=1e-5, max_value=1e-1, sampling=\"log\") \n",
    "    model = existing_model(units=units, dropout=dropout, lr=lr)\n",
    "    return model\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=keras_tuner.Objective(\"val_f1_score\", direction = \"max\"),\n",
    "    max_trials=3,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory=\"tmp\",\n",
    "    project_name=\"helloworld\"\n",
    ")\n",
    "\n",
    "\n",
    "build_model(keras_tuner.HyperParameters())\n",
    "\n",
    "tuner.search(x_train_new, y_train_new, epochs = 25, validation_data = (x, y), batch_size = 5000)\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def neural_network(x_train, y_train, x_test):\n",
    "    units = 48\n",
    "    lr = 0.001550047696969543835\n",
    "    dropout = True\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units = units, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "    if dropout:\n",
    "        keras.layers.Dropout(0.25)\n",
    "    model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss = 'binary_crossentropy', \n",
    "                      metrics = ['accuracy', 'F1Score', 'R2Score', keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    my_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor = 'val_f1_score', patience = 1, mode = 'max'), \n",
    "        # the learning rate will reduce once the F1Score has plateau-ed on a maximum value after 1 epoch\n",
    "        keras.callbacks.EarlyStopping(monitor = 'loss', mode = 'min', patience = 5)\n",
    "        # stops the training once loss is plateau-ed at the minimum\n",
    "    ] \n",
    "    \n",
    "\n",
    "    model.fit(x_train, y_train, epochs = 25, batch_size = 5000)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i][0] < 0.5:\n",
    "            y_pred[i][0] = 0\n",
    "        else:\n",
    "            y_pred[i][0] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "#     rmse = math.sqrt(metrics.mean_squared_error(y_pred,y_test))\n",
    "#     print(rmse)\n",
    "\n",
    "#     r2score = metrics.r2_score(y_pred, y_test)\n",
    "#     print(r2score)\n",
    "\n",
    "#     f1score = metrics.f1_score(y_pred, y_test)\n",
    "#     print(f1score)\n",
    "\n",
    "#     confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "#     cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "#     cm_display.plot()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    \n",
    "    df = hidden_data\n",
    "    \n",
    "    # Change the min_occ_date\n",
    "    \n",
    "    df[\"min_occ_date\"] = pd.to_datetime(df[\"min_occ_date\"],format = \"%Y-%m-%d\",errors='coerce')\n",
    "    df[\"cltdob_fix\"] = pd.to_datetime(df[\"cltdob_fix\"],format = \"%Y-%m-%d\",errors='coerce')\n",
    "\n",
    "    today = datetime.now()\n",
    "\n",
    "    df[\"days_since_first_purchase\"] = df[\"min_occ_date\"].transform(lambda x: today-x)\n",
    "    df[\"age\"] = df[\"cltdob_fix\"].transform(lambda x: today-x)\n",
    "\n",
    "    df[\"days_since_first_purchase\"] = pd.to_numeric(df['days_since_first_purchase'].dt.days, downcast='integer')\n",
    "    df[\"age\"] = pd.to_numeric((df['age'].dt.days)/365, downcast='integer')\n",
    "\n",
    "    df = df.drop(columns=[\"min_occ_date\",\"cltdob_fix\",\"clntnum\"])\n",
    "    \n",
    "    # Selection of data\n",
    "    \n",
    "    cols = df.columns\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "\n",
    "    df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    cols = df.columns\n",
    "    num_cols = df.select_dtypes(include=numerics)\n",
    "    cat_cols = df.select_dtypes(exclude=numerics)\n",
    "    \n",
    "    # Imputing and Encoding of data\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=2)\n",
    "    num_cols = pd.DataFrame(imputer.fit_transform(num_cols), columns=num_cols.columns)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    for col in cat_cols.columns:\n",
    "        encoded = le.fit_transform(cat_cols[col])    \n",
    "        cat_cols[col] = encoded\n",
    "\n",
    "    df = pd.concat([num_cols.reset_index(drop=True), cat_cols.reset_index(drop=True)], axis=1)\n",
    "    df = df.dropna(thresh=303)\n",
    "    \n",
    "    # Defining of input and output dataframes\n",
    "    \n",
    "    x = df.drop(columns=[\"f_purchase_lh\"])\n",
    "\n",
    "    y = df[\"f_purchase_lh\"]\n",
    "    \n",
    "    # Training 80%, test 20%\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ## Perform downsampling first on majority\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "\n",
    "    one_indices = y_train[y_train[\"f_purchase_lh\"]==1].index\n",
    "    one = len(y_train[y_train[\"f_purchase_lh\"]==1])\n",
    "\n",
    "    zero_indices = y_train[y_train[\"f_purchase_lh\"]==0].index\n",
    "    zero = len(y_train[y_train[\"f_purchase_lh\"]==0])\n",
    "\n",
    "    random_indices = np.random.choice(zero_indices, zero - 3000 , replace=False)\n",
    "\n",
    "    down_sample_indices = np.concatenate([one_indices,random_indices])\n",
    "\n",
    "    x_train_new = x_train.loc[down_sample_indices]\n",
    "    y_train_new = y_train.loc[down_sample_indices][\"f_purchase_lh\"]\n",
    "\n",
    "\n",
    "    ## Now do SMOTE on minority\n",
    "\n",
    "    x_train_new, y_train_new = SMOTE().fit_resample(x_train_new, y_train_new)\n",
    "\n",
    "\n",
    "    ## Normalise data\n",
    "    scaler = StandardScaler().fit(x_train_new)\n",
    "    x_train_new = scaler.transform(x_train_new)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x = scaler.transform(x)\n",
    "\n",
    "    result = neural_network(x_train_new, y_train_new, x_test)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5/5 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.5647 - f1_score: 0.6667 - r2_score: 0.0199 - root_mean_squared_error: 0.4950\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.6823 - f1_score: 0.6667 - r2_score: 0.1865 - root_mean_squared_error: 0.4510\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5456 - accuracy: 0.7103 - f1_score: 0.6667 - r2_score: 0.2664 - root_mean_squared_error: 0.4283\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5042 - accuracy: 0.7562 - f1_score: 0.6667 - r2_score: 0.3368 - root_mean_squared_error: 0.4072\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4686 - accuracy: 0.7847 - f1_score: 0.6667 - r2_score: 0.3930 - root_mean_squared_error: 0.3895\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.8023 - f1_score: 0.6667 - r2_score: 0.4423 - root_mean_squared_error: 0.3734\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4067 - accuracy: 0.8214 - f1_score: 0.6667 - r2_score: 0.4856 - root_mean_squared_error: 0.3586\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3802 - accuracy: 0.8350 - f1_score: 0.6667 - r2_score: 0.5241 - root_mean_squared_error: 0.3449\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3554 - accuracy: 0.8510 - f1_score: 0.6667 - r2_score: 0.5607 - root_mean_squared_error: 0.3314\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3328 - accuracy: 0.8628 - f1_score: 0.6667 - r2_score: 0.5931 - root_mean_squared_error: 0.3189\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8742 - f1_score: 0.6667 - r2_score: 0.6219 - root_mean_squared_error: 0.3074\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2951 - accuracy: 0.8846 - f1_score: 0.6667 - r2_score: 0.6469 - root_mean_squared_error: 0.2971\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2784 - accuracy: 0.8919 - f1_score: 0.6667 - r2_score: 0.6696 - root_mean_squared_error: 0.2874\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2641 - accuracy: 0.8980 - f1_score: 0.6667 - r2_score: 0.6891 - root_mean_squared_error: 0.2788\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2506 - accuracy: 0.9062 - f1_score: 0.6667 - r2_score: 0.7075 - root_mean_squared_error: 0.2704\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.9119 - f1_score: 0.6667 - r2_score: 0.7235 - root_mean_squared_error: 0.2629\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9158 - f1_score: 0.6667 - r2_score: 0.7372 - root_mean_squared_error: 0.2563\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2164 - accuracy: 0.9229 - f1_score: 0.6667 - r2_score: 0.7520 - root_mean_squared_error: 0.2490\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2060 - accuracy: 0.9275 - f1_score: 0.6667 - r2_score: 0.7657 - root_mean_squared_error: 0.2420\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1969 - accuracy: 0.9307 - f1_score: 0.6667 - r2_score: 0.7766 - root_mean_squared_error: 0.2363\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1875 - accuracy: 0.9350 - f1_score: 0.6668 - r2_score: 0.7887 - root_mean_squared_error: 0.2299\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9379 - f1_score: 0.6669 - r2_score: 0.7992 - root_mean_squared_error: 0.2241\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9426 - f1_score: 0.6669 - r2_score: 0.8107 - root_mean_squared_error: 0.2176\n",
      "Epoch 24/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.9464 - f1_score: 0.6669 - r2_score: 0.8204 - root_mean_squared_error: 0.2119\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1551 - accuracy: 0.9493 - f1_score: 0.6671 - r2_score: 0.8294 - root_mean_squared_error: 0.2065\n",
      "113/113 [==============================] - 0s 723us/step\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Epoch 1/25\n",
      "5/5 [==============================] - 1s 4ms/step - loss: 0.7110 - accuracy: 0.5447 - f1_score: 0.6667 - r2_score: -0.0233 - root_mean_squared_error: 0.5058\n",
      "Epoch 2/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6202 - accuracy: 0.6688 - f1_score: 0.6667 - r2_score: 0.1422 - root_mean_squared_error: 0.4631\n",
      "Epoch 3/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5723 - accuracy: 0.6901 - f1_score: 0.6667 - r2_score: 0.2210 - root_mean_squared_error: 0.4413\n",
      "Epoch 4/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5372 - accuracy: 0.7152 - f1_score: 0.6667 - r2_score: 0.2783 - root_mean_squared_error: 0.4248\n",
      "Epoch 5/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5077 - accuracy: 0.7470 - f1_score: 0.6667 - r2_score: 0.3284 - root_mean_squared_error: 0.4098\n",
      "Epoch 6/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.7737 - f1_score: 0.6667 - r2_score: 0.3740 - root_mean_squared_error: 0.3956\n",
      "Epoch 7/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4536 - accuracy: 0.7900 - f1_score: 0.6667 - r2_score: 0.4148 - root_mean_squared_error: 0.3825\n",
      "Epoch 8/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4286 - accuracy: 0.8040 - f1_score: 0.6667 - r2_score: 0.4525 - root_mean_squared_error: 0.3700\n",
      "Epoch 9/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4046 - accuracy: 0.8193 - f1_score: 0.6667 - r2_score: 0.4888 - root_mean_squared_error: 0.3575\n",
      "Epoch 10/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.8321 - f1_score: 0.6667 - r2_score: 0.5216 - root_mean_squared_error: 0.3458\n",
      "Epoch 11/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3610 - accuracy: 0.8447 - f1_score: 0.6667 - r2_score: 0.5534 - root_mean_squared_error: 0.3342\n",
      "Epoch 12/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3408 - accuracy: 0.8599 - f1_score: 0.6667 - r2_score: 0.5836 - root_mean_squared_error: 0.3227\n",
      "Epoch 13/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3219 - accuracy: 0.8704 - f1_score: 0.6667 - r2_score: 0.6111 - root_mean_squared_error: 0.3118\n",
      "Epoch 14/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.8797 - f1_score: 0.6667 - r2_score: 0.6353 - root_mean_squared_error: 0.3020\n",
      "Epoch 15/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2892 - accuracy: 0.8862 - f1_score: 0.6667 - r2_score: 0.6558 - root_mean_squared_error: 0.2933\n",
      "Epoch 16/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2756 - accuracy: 0.8922 - f1_score: 0.6667 - r2_score: 0.6743 - root_mean_squared_error: 0.2854\n",
      "Epoch 17/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.8983 - f1_score: 0.6667 - r2_score: 0.6912 - root_mean_squared_error: 0.2778\n",
      "Epoch 18/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2521 - accuracy: 0.9029 - f1_score: 0.6667 - r2_score: 0.7045 - root_mean_squared_error: 0.2718\n",
      "Epoch 19/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2416 - accuracy: 0.9072 - f1_score: 0.6667 - r2_score: 0.7178 - root_mean_squared_error: 0.2656\n",
      "Epoch 20/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9138 - f1_score: 0.6667 - r2_score: 0.7321 - root_mean_squared_error: 0.2588\n",
      "Epoch 21/25\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2216 - accuracy: 0.9173 - f1_score: 0.6667 - r2_score: 0.7442 - root_mean_squared_error: 0.2529\n",
      "Epoch 22/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2131 - accuracy: 0.9211 - f1_score: 0.6667 - r2_score: 0.7546 - root_mean_squared_error: 0.2477\n",
      "Epoch 23/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9247 - f1_score: 0.6667 - r2_score: 0.7667 - root_mean_squared_error: 0.2415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1957 - accuracy: 0.9282 - f1_score: 0.6667 - r2_score: 0.7765 - root_mean_squared_error: 0.2364\n",
      "Epoch 25/25\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9310 - f1_score: 0.6667 - r2_score: 0.7865 - root_mean_squared_error: 0.2310\n",
      "113/113 [==============================] - 0s 705us/step\n",
      "[337.]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "# test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
